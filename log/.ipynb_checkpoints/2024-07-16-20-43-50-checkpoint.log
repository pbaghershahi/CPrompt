2024-07-16,20:43:50 [INFO] Logging to: ./log/2024-07-16-20-43-50.log
2024-07-16,20:43:50 [INFO] Reading config from: ./config/dhfr.yaml
2024-07-16,20:43:50 [INFO] Namespace(total_iters=10, prompt_method='flex_match', s_dataset='DHFR', t_dataset='DHFR', prompt_fn='gpf_plus', shift_type='structural', p_shift_intra=0.0, p_shift_inter=0.0, pretrain=True, save_pretrained=False, soft_label=False, add_link_loss=False, attn_with_param=False, empty_pretrained_dir=False, iterative_clustering=False, clutering_iters=3, entropy_div_ratio=3, w_entropy_loss=0.0, w_softmax_loss=0.0, w_domain_loss=0.0, r_reg=0.0, gnn_weight_decay=0.0, weight_decay=0.0, gnn_type='gcn', gnn_num_layers=3, pretrained_path=[], gnn_n_epochs=100, gnn_eval_step=10, gnn_h_dim=512, gnn_lr=0.01, gnn_step_size=50, gnn_gamma=0.5, gnn_batch_size=64, gnn_dropout=0.4, n_epochs=50, eval_step=10, h_dim=512, lr=0.01, step_size=50, gamma=0.5, batch_size=64, num_tokens=20, src_ratio=0.5, dropout=0.3, cut_off=0.5, aug_type='feature', light_aug_prob=0.1, light_aug_mode='mask', pos_aug_mode='mask', neg_aug_mode='arbitrary', noise_cov_scale=1.0, noise_mean_shift=1.0, noise_shift_mode='homophily', noise_select_mode='soft', cross_prune=0.8, inner_prune=0.3, num_runs=5, p_raug=0.15, n_raug=0.15, s_split=[0.65, 0.3], t_split=[0.65, 0.3], seed=[], config_from_file='./config/dhfr.yaml', config_to_file='', label_reduction=0.0)
2024-07-16,20:43:50 [INFO] ####################################################################################################
2024-07-16,20:43:50 [INFO] Args seeds: [2391 2042 3619 3266 2897 2470 3543 2157 1811 1795]
2024-07-16,20:43:50 [INFO] Source Dataset: DHFR, Target Dataset: DHFR. Training, Test: Source: [0.65, 0.3], Target: [0.65, 0.3] -- Batch size: 64
2024-07-16,20:43:50 [INFO] All input pretrained paths: []
2024-07-16,20:43:50 [INFO] Started round 0/10 of experiments!
2024-07-16,20:43:50 [INFO] Setting for pretraining: Model: {'gnn_type': 'gcn', 'in_channels': 56, 'hidden_channels': 512, 'out_channels': 2, 'num_layers': 3, 'dropout': 0.4, 'with_bn': False, 'with_head': True} -- Optimizer: {'lr': 0.01, 'scheduler_step_size': 50, 'scheduler_gamma': 0.5, 'weight_decay': 0.0} -- Training: {'n_epochs': 100}
2024-07-16,20:43:50 [INFO] Pretraining GCN on DHFR started for 100 epochs
2024-07-16,20:43:51 [INFO] GNN Before Pretraining: -- Test Loss: 0.691 -- Test ACC: 0.583 -- Test F1-score: 0.736
2024-07-16,20:43:53 [INFO] Epoch: 10/100 -- Train Loss: 0.609 -- Validation Loss: 0.546 -- Validation ACC: 0.667 -- Validation F1: 0.800
2024-07-16,20:43:56 [INFO] Epoch: 20/100 -- Train Loss: 0.591 -- Validation Loss: 0.535 -- Validation ACC: 0.722 -- Validation F1: 0.783
2024-07-16,20:43:58 [INFO] Epoch: 30/100 -- Train Loss: 0.581 -- Validation Loss: 0.533 -- Validation ACC: 0.722 -- Validation F1: 0.783
2024-07-16,20:44:00 [INFO] Epoch: 40/100 -- Train Loss: 0.538 -- Validation Loss: 0.548 -- Validation ACC: 0.667 -- Validation F1: 0.727
2024-07-16,20:44:03 [INFO] Epoch: 50/100 -- Train Loss: 0.411 -- Validation Loss: 0.542 -- Validation ACC: 0.667 -- Validation F1: 0.727
2024-07-16,20:44:05 [INFO] Epoch: 60/100 -- Train Loss: 0.437 -- Validation Loss: 0.560 -- Validation ACC: 0.778 -- Validation F1: 0.833
2024-07-16,20:44:08 [INFO] Epoch: 70/100 -- Train Loss: 0.436 -- Validation Loss: 0.627 -- Validation ACC: 0.722 -- Validation F1: 0.783
2024-07-16,20:44:13 [INFO] Epoch: 80/100 -- Train Loss: 0.432 -- Validation Loss: 0.593 -- Validation ACC: 0.722 -- Validation F1: 0.783
2024-07-16,20:44:17 [INFO] Epoch: 90/100 -- Train Loss: 0.543 -- Validation Loss: 0.662 -- Validation ACC: 0.722 -- Validation F1: 0.783
2024-07-16,20:44:22 [INFO] GNN After Pretraining: -- Train Loss: 0.471 -- Test Loss: 0.641 -- Test ACC: 0.722 -- Test F1: 0.754
2024-07-16,20:44:22 [INFO] Model saved to: ./pretrained/GCN_Pretrained_2024-07-16-20-44-22.pth
2024-07-16,20:44:22 [INFO] Prompting method: flex_match -- Setting: Prompting function: gpf_plus -- Target Dataset: DHFR
2024-07-16,20:44:22 [INFO] Setting for prompt tuning: Prompt: {'emb_dim': 56, 'h_dim': 512, 'output_dim': 56, 'prompt_fn': 'gpf_plus', 'token_num': 20, 'cross_prune': 0.8, 'inner_prune': 0.3, 'attn_with_param': False} -- Pretrained Model: {'gnn_type': 'gcn', 'in_channels': 56, 'hidden_channels': 512, 'out_channels': 2, 'num_layers': 3, 'dropout': 0.4, 'with_bn': False, 'with_head': True} -- Optimizer: {'lr': 0.01, 'scheduler_step_size': 50, 'scheduler_gamma': 0.5, 'weight_decay': 0.0} -- Training: {'aug_type': 'feature', 'pos_aug_mode': 'mask', 'p_raug': 0.15, 'n_epochs': 50, 'r_reg': 0.0, 'soft_label': False, 'clutering_iters': 3, 'iterative_clustering': False, 'entropy_div_ratio': 3, 'w_entropy_loss': 0.0, 'w_softmax_loss': 0.0, 'w_domain_loss': 0.0, 'light_aug_prob': 0.1, 'light_aug_mode': 'mask', 'num_classes': 2, 'cut_off': 0.5}
2024-07-16,20:44:22 [INFO] Prompt tuning started: Num runs: 5 -- Eval step: 10
2024-07-16,20:44:22 [INFO] Pretrained GNN on Source Dataset -- Test Loss: 0.641 -- Test ACC: 0.722 -- Test F1-score: 0.754
2024-07-16,20:44:23 [INFO] Pretrained GNN on Target Dataset Without Prompting: -- Validation Loss: 0.436 -- Validation ACC: 0.833 -- Validation F1-score: 0.880 -- Test Loss: 0.563 -- Test ACC: 0.713 -- Test F1-score: 0.787
2024-07-16,20:44:26 [INFO] Epoch: 0/50 -- Train Loss: 0.631 -- Validation Loss: 0.485 -- Validation ACC: 0.833 -- Validation F1: 0.880
2024-07-16,20:44:40 [INFO] Epoch: 10/50 -- Train Loss: 0.483 -- Validation Loss: 0.434 -- Validation ACC: 0.833 -- Validation F1: 0.880
2024-07-16,20:44:56 [INFO] Epoch: 20/50 -- Train Loss: 0.490 -- Validation Loss: 0.416 -- Validation ACC: 0.833 -- Validation F1: 0.880
