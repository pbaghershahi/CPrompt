2024-07-16,20:44:06 [INFO] Logging to: ./log/2024-07-16-20-44-06.log
2024-07-16,20:44:06 [INFO] Reading config from: ./config/dhfr.yaml
2024-07-16,20:44:06 [INFO] Namespace(total_iters=10, prompt_method='flex_match', s_dataset='DHFR', t_dataset='DHFR', prompt_fn='gpf_plus', shift_type='structural', p_shift_intra=0.0, p_shift_inter=0.0, pretrain=True, save_pretrained=False, soft_label=False, add_link_loss=False, attn_with_param=False, empty_pretrained_dir=False, iterative_clustering=False, clutering_iters=3, entropy_div_ratio=3, w_entropy_loss=0.0, w_softmax_loss=0.0, w_domain_loss=0.0, r_reg=0.0, gnn_weight_decay=0.0, weight_decay=0.0, gnn_type='gcn', gnn_num_layers=3, pretrained_path=[], gnn_n_epochs=100, gnn_eval_step=10, gnn_h_dim=512, gnn_lr=0.01, gnn_step_size=50, gnn_gamma=0.5, gnn_batch_size=64, gnn_dropout=0.4, n_epochs=50, eval_step=10, h_dim=512, lr=0.01, step_size=50, gamma=0.5, batch_size=64, num_tokens=10, src_ratio=0.5, dropout=0.3, cut_off=0.5, aug_type='feature', light_aug_prob=0.1, light_aug_mode='mask', pos_aug_mode='mask', neg_aug_mode='arbitrary', noise_cov_scale=1.0, noise_mean_shift=1.0, noise_shift_mode='homophily', noise_select_mode='soft', cross_prune=0.8, inner_prune=0.3, num_runs=5, p_raug=0.15, n_raug=0.15, s_split=[0.65, 0.3], t_split=[0.65, 0.3], seed=[], config_from_file='./config/dhfr.yaml', config_to_file='', label_reduction=0.0)
2024-07-16,20:44:06 [INFO] ####################################################################################################
2024-07-16,20:44:06 [INFO] Args seeds: [1420 1774 1559 4239 1682 3826 3447 2067 4843 3279]
2024-07-16,20:44:06 [INFO] Source Dataset: DHFR, Target Dataset: DHFR. Training, Test: Source: [0.65, 0.3], Target: [0.65, 0.3] -- Batch size: 64
2024-07-16,20:44:06 [INFO] All input pretrained paths: []
2024-07-16,20:44:06 [INFO] Started round 0/10 of experiments!
2024-07-16,20:44:06 [INFO] Setting for pretraining: Model: {'gnn_type': 'gcn', 'in_channels': 56, 'hidden_channels': 512, 'out_channels': 2, 'num_layers': 3, 'dropout': 0.4, 'with_bn': False, 'with_head': True} -- Optimizer: {'lr': 0.01, 'scheduler_step_size': 50, 'scheduler_gamma': 0.5, 'weight_decay': 0.0} -- Training: {'n_epochs': 100}
2024-07-16,20:44:06 [INFO] Pretraining GCN on DHFR started for 100 epochs
2024-07-16,20:44:07 [INFO] GNN Before Pretraining: -- Test Loss: 0.690 -- Test ACC: 0.600 -- Test F1-score: 0.750
2024-07-16,20:44:13 [INFO] Epoch: 10/100 -- Train Loss: 0.602 -- Validation Loss: 0.667 -- Validation ACC: 0.667 -- Validation F1: 0.727
2024-07-16,20:44:17 [INFO] Epoch: 20/100 -- Train Loss: 0.475 -- Validation Loss: 0.523 -- Validation ACC: 0.778 -- Validation F1: 0.750
2024-07-16,20:44:22 [INFO] Epoch: 30/100 -- Train Loss: 0.366 -- Validation Loss: 0.561 -- Validation ACC: 0.722 -- Validation F1: 0.737
2024-07-16,20:44:27 [INFO] Epoch: 40/100 -- Train Loss: 0.239 -- Validation Loss: 0.586 -- Validation ACC: 0.722 -- Validation F1: 0.762
2024-07-16,20:44:32 [INFO] Epoch: 50/100 -- Train Loss: 0.339 -- Validation Loss: 0.508 -- Validation ACC: 0.667 -- Validation F1: 0.700
2024-07-16,20:44:37 [INFO] Epoch: 60/100 -- Train Loss: 0.279 -- Validation Loss: 0.692 -- Validation ACC: 0.778 -- Validation F1: 0.818
2024-07-16,20:44:42 [INFO] Epoch: 70/100 -- Train Loss: 0.223 -- Validation Loss: 0.562 -- Validation ACC: 0.667 -- Validation F1: 0.625
2024-07-16,20:44:47 [INFO] Epoch: 80/100 -- Train Loss: 0.323 -- Validation Loss: 0.545 -- Validation ACC: 0.722 -- Validation F1: 0.706
2024-07-16,20:44:52 [INFO] Epoch: 90/100 -- Train Loss: 0.272 -- Validation Loss: 0.552 -- Validation ACC: 0.667 -- Validation F1: 0.700
2024-07-16,20:44:57 [INFO] GNN After Pretraining: -- Train Loss: 0.268 -- Test Loss: 1.064 -- Test ACC: 0.696 -- Test F1: 0.762
2024-07-16,20:44:57 [INFO] Model saved to: ./pretrained/GCN_Pretrained_2024-07-16-20-44-57.pth
2024-07-16,20:44:57 [INFO] Prompting method: flex_match -- Setting: Prompting function: gpf_plus -- Target Dataset: DHFR
2024-07-16,20:44:57 [INFO] Setting for prompt tuning: Prompt: {'emb_dim': 56, 'h_dim': 512, 'output_dim': 56, 'prompt_fn': 'gpf_plus', 'token_num': 10, 'cross_prune': 0.8, 'inner_prune': 0.3, 'attn_with_param': False} -- Pretrained Model: {'gnn_type': 'gcn', 'in_channels': 56, 'hidden_channels': 512, 'out_channels': 2, 'num_layers': 3, 'dropout': 0.4, 'with_bn': False, 'with_head': True} -- Optimizer: {'lr': 0.01, 'scheduler_step_size': 50, 'scheduler_gamma': 0.5, 'weight_decay': 0.0} -- Training: {'aug_type': 'feature', 'pos_aug_mode': 'mask', 'p_raug': 0.15, 'n_epochs': 50, 'r_reg': 0.0, 'soft_label': False, 'clutering_iters': 3, 'iterative_clustering': False, 'entropy_div_ratio': 3, 'w_entropy_loss': 0.0, 'w_softmax_loss': 0.0, 'w_domain_loss': 0.0, 'light_aug_prob': 0.1, 'light_aug_mode': 'mask', 'num_classes': 2, 'cut_off': 0.5}
2024-07-16,20:44:57 [INFO] Prompt tuning started: Num runs: 5 -- Eval step: 10
2024-07-16,20:44:58 [INFO] Pretrained GNN on Source Dataset -- Test Loss: 1.064 -- Test ACC: 0.696 -- Test F1-score: 0.762
2024-07-16,20:44:59 [INFO] Pretrained GNN on Target Dataset Without Prompting: -- Validation Loss: 0.989 -- Validation ACC: 0.611 -- Validation F1-score: 0.696 -- Test Loss: 0.836 -- Test ACC: 0.730 -- Test F1-score: 0.774
2024-07-16,20:45:01 [INFO] Epoch: 0/50 -- Train Loss: 0.771 -- Validation Loss: 0.917 -- Validation ACC: 0.611 -- Validation F1: 0.667
