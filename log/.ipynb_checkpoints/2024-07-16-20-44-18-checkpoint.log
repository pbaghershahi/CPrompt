2024-07-16,20:44:18 [INFO] Logging to: ./log/2024-07-16-20-44-18.log
2024-07-16,20:44:18 [INFO] Reading config from: ./config/dhfr.yaml
2024-07-16,20:44:18 [INFO] Namespace(total_iters=10, prompt_method='flex_match', s_dataset='DHFR', t_dataset='DHFR', prompt_fn='gpf_plus', shift_type='structural', p_shift_intra=0.0, p_shift_inter=0.0, pretrain=True, save_pretrained=False, soft_label=False, add_link_loss=False, attn_with_param=False, empty_pretrained_dir=False, iterative_clustering=False, clutering_iters=3, entropy_div_ratio=3, w_entropy_loss=0.0, w_softmax_loss=0.0, w_domain_loss=0.0, r_reg=0.0, gnn_weight_decay=0.0, weight_decay=0.0, gnn_type='gcn', gnn_num_layers=3, pretrained_path=[], gnn_n_epochs=100, gnn_eval_step=10, gnn_h_dim=512, gnn_lr=0.01, gnn_step_size=50, gnn_gamma=0.5, gnn_batch_size=64, gnn_dropout=0.4, n_epochs=50, eval_step=10, h_dim=512, lr=0.01, step_size=50, gamma=0.5, batch_size=64, num_tokens=30, src_ratio=0.5, dropout=0.3, cut_off=0.5, aug_type='feature', light_aug_prob=0.1, light_aug_mode='mask', pos_aug_mode='mask', neg_aug_mode='arbitrary', noise_cov_scale=1.0, noise_mean_shift=1.0, noise_shift_mode='homophily', noise_select_mode='soft', cross_prune=0.8, inner_prune=0.3, num_runs=5, p_raug=0.15, n_raug=0.15, s_split=[0.65, 0.3], t_split=[0.65, 0.3], seed=[], config_from_file='./config/dhfr.yaml', config_to_file='', label_reduction=0.0)
2024-07-16,20:44:18 [INFO] ####################################################################################################
2024-07-16,20:44:18 [INFO] Args seeds: [4019 2053 3208 1620 4578 4946 1871 2385 2485 4891]
2024-07-16,20:44:18 [INFO] Source Dataset: DHFR, Target Dataset: DHFR. Training, Test: Source: [0.65, 0.3], Target: [0.65, 0.3] -- Batch size: 64
2024-07-16,20:44:18 [INFO] All input pretrained paths: []
2024-07-16,20:44:18 [INFO] Started round 0/10 of experiments!
2024-07-16,20:44:19 [INFO] Setting for pretraining: Model: {'gnn_type': 'gcn', 'in_channels': 56, 'hidden_channels': 512, 'out_channels': 2, 'num_layers': 3, 'dropout': 0.4, 'with_bn': False, 'with_head': True} -- Optimizer: {'lr': 0.01, 'scheduler_step_size': 50, 'scheduler_gamma': 0.5, 'weight_decay': 0.0} -- Training: {'n_epochs': 100}
2024-07-16,20:44:19 [INFO] Pretraining GCN on DHFR started for 100 epochs
2024-07-16,20:44:21 [INFO] GNN Before Pretraining: -- Test Loss: 0.695 -- Test ACC: 0.435 -- Test F1-score: 0.198
2024-07-16,20:44:27 [INFO] Epoch: 10/100 -- Train Loss: 0.632 -- Validation Loss: 0.603 -- Validation ACC: 0.833 -- Validation F1: 0.824
2024-07-16,20:44:32 [INFO] Epoch: 20/100 -- Train Loss: 0.628 -- Validation Loss: 0.758 -- Validation ACC: 0.444 -- Validation F1: 0.615
2024-07-16,20:44:37 [INFO] Epoch: 30/100 -- Train Loss: 0.509 -- Validation Loss: 0.656 -- Validation ACC: 0.778 -- Validation F1: 0.800
2024-07-16,20:44:42 [INFO] Epoch: 40/100 -- Train Loss: 0.408 -- Validation Loss: 0.947 -- Validation ACC: 0.611 -- Validation F1: 0.696
2024-07-16,20:44:47 [INFO] Epoch: 50/100 -- Train Loss: 0.385 -- Validation Loss: 1.051 -- Validation ACC: 0.667 -- Validation F1: 0.727
2024-07-16,20:44:52 [INFO] Epoch: 60/100 -- Train Loss: 0.325 -- Validation Loss: 0.998 -- Validation ACC: 0.778 -- Validation F1: 0.800
2024-07-16,20:44:58 [INFO] Epoch: 70/100 -- Train Loss: 0.343 -- Validation Loss: 1.128 -- Validation ACC: 0.611 -- Validation F1: 0.696
2024-07-16,20:45:02 [INFO] Epoch: 80/100 -- Train Loss: 0.233 -- Validation Loss: 1.516 -- Validation ACC: 0.667 -- Validation F1: 0.727
2024-07-16,20:45:06 [INFO] Epoch: 90/100 -- Train Loss: 0.253 -- Validation Loss: 1.677 -- Validation ACC: 0.722 -- Validation F1: 0.762
2024-07-16,20:45:10 [INFO] GNN After Pretraining: -- Train Loss: 0.230 -- Test Loss: 2.435 -- Test ACC: 0.696 -- Test F1: 0.741
2024-07-16,20:45:10 [INFO] Model saved to: ./pretrained/GCN_Pretrained_2024-07-16-20-45-10.pth
2024-07-16,20:45:10 [INFO] Prompting method: flex_match -- Setting: Prompting function: gpf_plus -- Target Dataset: DHFR
2024-07-16,20:45:10 [INFO] Setting for prompt tuning: Prompt: {'emb_dim': 56, 'h_dim': 512, 'output_dim': 56, 'prompt_fn': 'gpf_plus', 'token_num': 30, 'cross_prune': 0.8, 'inner_prune': 0.3, 'attn_with_param': False} -- Pretrained Model: {'gnn_type': 'gcn', 'in_channels': 56, 'hidden_channels': 512, 'out_channels': 2, 'num_layers': 3, 'dropout': 0.4, 'with_bn': False, 'with_head': True} -- Optimizer: {'lr': 0.01, 'scheduler_step_size': 50, 'scheduler_gamma': 0.5, 'weight_decay': 0.0} -- Training: {'aug_type': 'feature', 'pos_aug_mode': 'mask', 'p_raug': 0.15, 'n_epochs': 50, 'r_reg': 0.0, 'soft_label': False, 'clutering_iters': 3, 'iterative_clustering': False, 'entropy_div_ratio': 3, 'w_entropy_loss': 0.0, 'w_softmax_loss': 0.0, 'w_domain_loss': 0.0, 'light_aug_prob': 0.1, 'light_aug_mode': 'mask', 'num_classes': 2, 'cut_off': 0.5}
2024-07-16,20:45:10 [INFO] Prompt tuning started: Num runs: 5 -- Eval step: 10
2024-07-16,20:45:10 [INFO] Pretrained GNN on Source Dataset -- Test Loss: 2.435 -- Test ACC: 0.696 -- Test F1-score: 0.741
2024-07-16,20:45:11 [INFO] Pretrained GNN on Target Dataset Without Prompting: -- Validation Loss: 0.661 -- Validation ACC: 0.778 -- Validation F1-score: 0.800 -- Test Loss: 0.772 -- Test ACC: 0.730 -- Test F1-score: 0.767
2024-07-16,20:45:12 [INFO] Epoch: 0/50 -- Train Loss: 0.056 -- Validation Loss: 0.577 -- Validation ACC: 0.833 -- Validation F1: 0.842
